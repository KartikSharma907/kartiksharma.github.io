<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kartik Sharma</title>

    <meta name="author" content="Kartik Sharma">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/site-logo.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kartik Sharma
                </p>
                
                <p>
                  I am an incoming MSR student at Carnegie Mellon University (Fall 2025), interested in building intelligent systems that learn from — and reason across — diverse forms of data. Previously I was a senior engineer at Samsung R&D Institute in Bangalore, where I worked in Language AI team on building safety filter for Samsung's LLM and LVM. I was also responsible for deploying on-device LLM and LVM models on Samsung's flagship devices.
                </p>
                <p>
                  I did my undergrad thesis at <a href="https://val.cds.iisc.ac.in" target="_blank">Vision and AI Lab</a> 
                  supervised by Prof. <a href="https://cds.iisc.ac.in/faculty/venky/" target="_blank">R. Venkatesh Babu</a>, 
                  where I worked on long-tail image generation using StyleGANs. I have also collaborated with 
                  <a href="https://www.bits-pilani.ac.in/pilani/poonam-goyal/" target="_blank">Prof. Poonam Goyal</a> 
                  in <a href="https://www.bits-pilani.ac.in/advanced-data-analytics-parallel-technologies-laboratory/" target="_blank">ADAPT Lab</a> 
                  where I worked on a generalized multimodal approach for early crop-yield prediction. My research spans 
                  computer vision, natural language processing, and multi-modal deep learning, with a strong focus on making 
                  AI systems more ethical, transparent, and deployable in real-world settings.
                </p>
                <p>
                  Right now, I'm looking forward to expand my expertise in reinforcement learning, multi-modal reasoning and its applications in robotics — aiming to design systems that simplify human tasks through intelligent perception and decision-making. My goal is to bridge research with impactful applications, ensuring AI benefits people at scale. I did my undergrad at BITS Pilani, in Computer Science and Economics (dual degree).
                </p>
                <p style="text-align:center">
                  <a href="mailto:f20180789@pilani.bits-pilani.ac.in">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Kartik Sharma.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/KartikSharma907">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.es/citations?user=sGYGD4wAAAAJ&hl=sv">Scholar</a> &nbsp;/&nbsp;
                  <a href="www.linkedin.com/in/kartik-sharmma-69b578179">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/bio-img.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/bio-img.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am fascinated by how humans integrate different sensory signals — sight, sound, language — to understand and interact with the world. My research aims to replicate this capability in artificial systems, enabling them to process and reason across multiple modalities of data. This includes advancing methods in multi-modal deep learning for tasks like language-vision reasoning, domain adaptation in real-world environments, and robust performance in long-tailed data distributions. I also aim to embed ethical frameworks into AI, ensuring fairness, transparency, and alignment with societal values.
                </p>
                <p>
                  My work has explored diverse methodologies — from contrastive learning for cross-lingual object grounding, to GAN-based strategies for generating diverse images in limited data scenarios, to building RAG pipelines for large language models. I focus on designing architectures and training strategies that improve generalization, reduce biases, and scale efficiently to deployment.
                </p>
                <h3>Current & Future Work</h3>
                <p>
                  Currently, I'm looking forward to pursue research at CMU as a Robotics master's student in the domain of reinforcement learning and multi-modal reasoning for robotics — integrating perception, language understanding, and decision-making, to create systems that can adapt to dynamic environments. I am particularly interested in aligning diffusion and GAN-based generation models with reward signals for more controllable outputs, and in developing multi-modal architectures capable of reliable reasoning across diverse contexts. Looking ahead, I plan to explore how these systems can be deployed in the physical world, with a focus on assisting humans in high-stakes and complex tasks where safety, adaptability, and transparency are required. 
                </p>
                <p>
                  I am open to exploring new research directions as well, so please feel free to reach out!
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="noisytwins_stop()" onmouseover="noisytwins_start()" bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href='images/NoisyTwins-poster-A0.png' target="_blank" rel="noopener">
                  <div class="one">
                    <div class="two" id='noisytwins_image'><img src='images/NoisyTwins-poster-A0.png' width="160"></div>
                    <img src='images/NoisyTwins-poster-A0.png' width="160">
                  </div>
                </a>
                <script type="text/javascript">
                  function noisytwins_start() {
                    document.getElementById('noisytwins_image').style.opacity = "1";
                  }

                  function noisytwins_stop() {
                    document.getElementById('noisytwins_image').style.opacity = "0";
                  }
                  noisytwins_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs</span><br>
                Harsh Rangwani, Lavish Bansal, Kartik Sharma, Tejan Karmali, Varun Jampani, R. Venkatesh Babu<br>
                <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</em><br>
                <a href="https://arxiv.org/pdf/2304.05866">Link to Paper</a> /
                <a href="https://rangwani-harsh.github.io/NoisyTwins/">Project Page</a> /
                <a href="https://github.com/val-iisc/NoisyTwins">Code</a>
              </td>
            </tr>

            <tr onmouseout="crop_yield_stop()" onmouseover="crop_yield_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href='images/CropYieldNet-arch.png' target="_blank" rel="noopener">
                  <div class="one">
                    <div class="two" id='crop_yield_image'><img src='images/CropYieldNet-arch.png' width="160"></div>
                    <img src='images/CropYieldNet-arch.png' width="160">
                  </div>
                </a>
                <script type="text/javascript">
                  function crop_yield_start() {
                    document.getElementById('crop_yield_image').style.opacity = "1";
                  }

                  function crop_yield_stop() {
                    document.getElementById('crop_yield_image').style.opacity = "0";
                  }
                  crop_yield_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">A Generalized Multimodal Deep Learning Model for Early Crop Yield Prediction</span><br>
                Arshveer Kaur, Poonam Goyal, Kartik Sharma, Lakshay Sharma, Navneet Goyal<br>
                <em>IEEE International Conference on Big Data (Big Data 2022)</em><br>
                <a href="https://www.researchgate.net/profile/Kartik-Sharma-35/publication/367456295_A_Generalized_Multimodal_Deep_Learning_Model_for_Early_Crop_Yield_Prediction/links/668311410a25e27fbc1a78a9/A-Generalized-Multimodal-Deep-Learning-Model-for-Early-Crop-Yield-Prediction.pdf">Link to Paper</a>
              </td>
            </tr>
          </tbody></table>

          <!-- Experience Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
                <p>
                  <strong>Research Engineer</strong> — Samsung R&D Institute India (Nov 2023 – Present)<br>
                  Developed a multilingual safety filter for Samsung’s LLM with 95% accuracy across 12+ locales, deployed on flagship devices with 45% smaller models. Improved cross-lingual image grounding for better object localization in low-resource languages.                </p>
                <p>
                  <strong>Data Scientist</strong> — PrivateBlok (Feb 2023 – Nov 2023)<br>
                  Built PrivateBlok's MVP, a GPT-3.5-powered financial QA chatbot for 10K+ companies. Enhanced retrieval accuracy with a custom re-ranking algorithm and created a temporal knowledge graph for detailed financial insights.                </p>
                <p>
                  <strong>Project Assistant</strong> — Video Analytics Lab, IISc Bangalore (Aug 2022 – Jan 2023)<br>
                  Improved long-tailed image generation using StyleGANs, achieving 19% better FID scores. Published work at CVPR 2023, setting a new state-of-the-art for long-tailed datasets like ImageNet-LT, CIFAR10-LT, and iNaturalist-LT.               </p>
                <p>
                  <strong>Software R&D Intern</strong> — Samsung R&D Institute India (May 2022 – Jul 2022)<br>
                  Developed an object-action detection system with 82% precision and optimized transformers for large-scale action recognition.               </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Independent Projects Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Independent Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="wakeup_word_stop()" onmouseover="wakeup_word_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/wake_word_detect.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='wakeup_word_image'><img src='images/wake_word_detect.png' width="160"></div>
            <img src='images/wake_word_detect.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function wakeup_word_start() {
            document.getElementById('wakeup_word_image').style.opacity = "1";
          }

          function wakeup_word_stop() {
            document.getElementById('wakeup_word_image').style.opacity = "0";
          }
          wakeup_word_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">
          <span class="papertitle">Wake-up Word Detection</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, July 2021
        <br>
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">code</a>
        <p>Constructed a speech dataset from synthesized data and implemented a trigger word detection model with over 90% accuracy. Trained a GRU(Gated Recurrent Units) to detect when someone has finished saying the word "activate".</p>
      </td>
    </tr>

    <tr onmouseout="car_detect_stop()" onmouseover="car_detect_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/car_detect_yolo.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='car_detect_image'><img src='images/car_detect_yolo.png' width="160"></div>
            <img src='images/car_detect_yolo.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function car_detect_start() {
            document.getElementById('car_detect_image').style.opacity = "1";
          }

          function car_detect_stop() {
            document.getElementById('car_detect_image').style.opacity = "0";
          }
          car_detect_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">
          <span class="papertitle">Car Detection with YOLO: You Only Look Once</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">code</a>
        <p>Implemented real-time object detection on a car dataset using the YOLO model, which was further improved using a U-net architecture. The YOLO model was stacked with Non-max suppression layers using IOU grid analysis to obtain the most accurate boundary boxes.</p>
      </td>
    </tr>

    <tr onmouseout="nst_stop()" onmouseover="nst_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/nst.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='nst_image'><img src='images/nst.png' width="160"></div>
            <img src='images/nst.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function nst_start() {
            document.getElementById('nst_image').style.opacity = "1";
          }

          function nst_stop() {
            document.getElementById('nst_image').style.opacity = "0";
          }
          nst_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">
          <span class="papertitle">Art Generation with Neural Style Transfer</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">code</a>
        <p>Used transfer learning on the VGG-19 network to generate new artistic images. Implemented a cost function that minimizes the content and style cost by running both the images through the pre-trained VGG-19 model.</p>
      </td>
    </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you for visiting my site, and if you have any inputs, I would love to hear from you!
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron.github.io" target="_blank">source code</a>. Do not scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website — use the github code instead.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
