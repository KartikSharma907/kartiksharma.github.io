<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kartik Sharma</title>

    <meta name="author" content="Kartik Sharma">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/site-logo.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kartik Sharma
                </p>

                <p>
                  I’m an incoming MSR student at Carnegie Mellon University (Fall ’25) focused on building intelligent systems that learn and reason across modalities. 
                  Previously, I was a Senior Engineer at Samsung R&D Bangalore, where I built safety filters for Samsung’s LLMs/LVMs and shipped on-device models to flagship devices. 
                  I earned a dual degree in Computer Science & Economics from BITS Pilani, completing research with Prof. <a href="https://cds.iisc.ac.in/faculty/venky/" target="_blank">R. Venkatesh Babu</a> (Long-tail Image Generation with StyleGANs) and Prof. <a href="https://www.bits-pilani.ac.in/pilani/poonam-goyal/" target="_blank">Prof. Poonam Goyal</a> (Multimodal Crop-yield Prediction). 
                  I now aim to advance RL + Multimodal reasoning for robotics with safe, transparent, real-world deployment.
                </p>
                
                <!-- <p>
                  I am an incoming MSR student at Carnegie Mellon University (Fall 2025), interested in building intelligent systems that learn from — and reason across — diverse forms of data. Previously I was a Senior Engineer at Samsung R&D Institute in Bangalore, where I worked in Language AI team on building safety filter for Samsung's LLM and LVM. I was also responsible for deploying on-device LLM and LVM models on Samsung's flagship devices.
                </p> -->
                <!-- <p>
                  I did my undergrad thesis at <a href="https://val.cds.iisc.ac.in" target="_blank">Vision and AI Lab</a> 
                  supervised by Prof. <a href="https://cds.iisc.ac.in/faculty/venky/" target="_blank">R. Venkatesh Babu</a>, 
                  where I worked on long-tail image generation using StyleGANs. I have also collaborated with 
                  <a href="https://www.bits-pilani.ac.in/pilani/poonam-goyal/" target="_blank">Prof. Poonam Goyal</a> 
                  in <a href="https://www.bits-pilani.ac.in/advanced-data-analytics-parallel-technologies-laboratory/" target="_blank">ADAPT Lab</a> 
                  where I worked on a generalized multimodal approach for early crop-yield prediction. My research spans 
                  computer vision, natural language processing, and multi-modal deep learning, with a strong focus on making 
                  AI systems more ethical, transparent, and deployable in real-world settings.
                </p> -->
                <!-- <p>
                  Right now, I'm looking forward to expand my expertise in reinforcement learning, multi-modal reasoning and its applications in robotics — aiming to design systems that simplify human tasks through intelligent perception and decision-making. My goal is to bridge research with impactful applications, ensuring AI benefits people at scale. I did my undergrad at BITS Pilani, in Computer Science and Economics (dual degree).
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:kartiksh@andrew.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Kartik_Sharma.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://github.com/KartikSharma907">Github</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.es/citations?user=sGYGD4wAAAAJ&hl=sv">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kartik-sharmma-69b578179/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/bio-img.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/bio-img.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests are broadly at the intersection of vision–language, robotics, and reinforcement learning. 
                  I am fascinated by how humans integrate multiple sensory signals—sight, sound, and language—to reason and act in the world, and I aim to replicate this ability in artificial systems. 
                  I study multimodal learning and reasoning, focusing on fusing vision, language, and action to enable robust decision-making in dynamic environments. 
                  My prior work spans language–vision reasoning, domain adaptation under distribution shift, and robustness in long-tailed settings, leveraging tools such as contrastive learning for cross-lingual grounding, generative models (GANs/diffusion) for limited-data scenarios, and retrieval-augmented pipelines for LLMs. 
                  Going forward, I am particularly interested in exploring methods to incorporate human priors by learning transferable skill representations across humans and robots. </p>
                <!-- <p>
                  I am fascinated by how humans integrate different sensory signals — sight, sound, language — to understand and interact with the world. My research aims to replicate this capability in artificial systems, enabling them to process and reason across multiple modalities of data. This includes advancing methods in multi-modal deep learning for tasks like language-vision reasoning, domain adaptation in real-world environments, and robust performance in long-tailed data distributions. I also aim to embed ethical frameworks into AI, ensuring fairness, transparency, and alignment with societal values.
                </p>
                <p>
                  My work has explored diverse methodologies — from contrastive learning for cross-lingual object grounding, to GAN-based strategies for generating diverse images in limited data scenarios, to building RAG pipelines for large language models. I focus on designing architectures and training strategies that improve generalization, reduce biases, and scale efficiently to deployment.
                </p>
                <h3>Current & Future Work</h3>
                <p>
                  Currently, I'm looking forward to pursue research at CMU as a Robotics master's student in the domain of reinforcement learning and multi-modal reasoning for robotics — integrating perception, language understanding, and decision-making, to create systems that can adapt to dynamic environments. I am particularly interested in aligning diffusion and GAN-based generation models with reward signals for more controllable outputs, and in developing multi-modal architectures capable of reliable reasoning across diverse contexts. Looking ahead, I plan to explore how these systems can be deployed in the physical world, with a focus on assisting humans in high-stakes and complex tasks where safety, adaptability, and transparency are required. 
                </p> -->
                <p>
                  I am open to exploring new research directions as well, so please feel free to reach out!
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="noisytwins_stop()" onmouseover="noisytwins_start()" bgcolor="#ffffd0">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href='images/NoisyTwins-poster-A0.png' target="_blank" rel="noopener">
                  <div class="one">
                    <div class="two" id='noisytwins_image'><img src='images/NoisyTwins-poster-A0.png' width="160"></div>
                    <img src='images/NoisyTwins-poster-A0.png' width="160">
                  </div>
                </a>
                <script type="text/javascript">
                  function noisytwins_start() {
                    document.getElementById('noisytwins_image').style.opacity = "1";
                  }

                  function noisytwins_stop() {
                    document.getElementById('noisytwins_image').style.opacity = "0";
                  }
                  noisytwins_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs</span><br>
                Harsh Rangwani, Lavish Bansal, Kartik Sharma, Tejan Karmali, Varun Jampani, R. Venkatesh Babu<br>
                <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</em><br>
                <a href="https://arxiv.org/pdf/2304.05866">Link to Paper</a> /
                <a href="https://rangwani-harsh.github.io/NoisyTwins/">Project Page</a> /
                <a href="https://github.com/val-iisc/NoisyTwins">Code</a>
              </td>
            </tr>

            <tr onmouseout="crop_yield_stop()" onmouseover="crop_yield_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <a href='images/CropYieldNet-arch.png' target="_blank" rel="noopener">
                  <div class="one">
                    <div class="two" id='crop_yield_image'><img src='images/CropYieldNet-arch.png' width="160"></div>
                    <img src='images/CropYieldNet-arch.png' width="160">
                  </div>
                </a>
                <script type="text/javascript">
                  function crop_yield_start() {
                    document.getElementById('crop_yield_image').style.opacity = "1";
                  }

                  function crop_yield_stop() {
                    document.getElementById('crop_yield_image').style.opacity = "0";
                  }
                  crop_yield_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">A Generalized Multimodal Deep Learning Model for Early Crop Yield Prediction</span><br>
                Arshveer Kaur, Poonam Goyal, Kartik Sharma, Lakshay Sharma, Navneet Goyal<br>
                <em>IEEE International Conference on Big Data (Big Data 2022)</em><br>
                <a href="https://www.researchgate.net/profile/Kartik-Sharma-35/publication/367456295_A_Generalized_Multimodal_Deep_Learning_Model_for_Early_Crop_Yield_Prediction/links/668311410a25e27fbc1a78a9/A-Generalized-Multimodal-Deep-Learning-Model-for-Early-Crop-Yield-Prediction.pdf">Link to Paper</a>
              </td>
            </tr>
          </tbody></table>

          <!-- Experience Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
                <p>
                  <strong>Research Engineer</strong> — Samsung R&D Institute India (Nov 2023 – May 2025)<br>
                  Developed a multilingual safety filter for Samsung’s LLM with 95% accuracy across 12+ locales, deployed on flagship devices with 45% smaller models. Improved cross-lingual image grounding for better object localization in low-resource languages.                </p>
                <p>
                  <strong>Data Scientist</strong> — PrivateBlok (Feb 2023 – Nov 2023)<br>
                  Built PrivateBlok's MVP, a GPT-3.5-powered financial QA chatbot for 10K+ companies. Enhanced retrieval accuracy with a custom re-ranking algorithm and created a temporal knowledge graph for detailed financial insights.                </p>
                <p>
                  <strong>Project Assistant</strong> — Video Analytics Lab, IISc Bangalore (Aug 2022 – Jan 2023)<br>
                  Improved long-tailed image generation using StyleGANs, achieving 19% better FID scores. Published work at CVPR 2023, setting a new state-of-the-art for long-tailed datasets like ImageNet-LT, CIFAR10-LT, and iNaturalist-LT.               </p>
                <p>
                  <strong>Software R&D Intern</strong> — Samsung R&D Institute India (May 2022 – Jul 2022)<br>
                  Developed an object-action detection system with 82% precision and optimized transformers for large-scale action recognition.               </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Independent Projects Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Independent Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="wakeup_word_stop()" onmouseover="wakeup_word_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/wake_word_detect.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='wakeup_word_image'><img src='images/wake_word_detect.png' width="160"></div>
            <img src='images/wake_word_detect.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function wakeup_word_start() {
            document.getElementById('wakeup_word_image').style.opacity = "1";
          }

          function wakeup_word_stop() {
            document.getElementById('wakeup_word_image').style.opacity = "0";
          }
          wakeup_word_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">
          <span class="papertitle">Wake-up Word Detection</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, July 2021
        <br>
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">code</a>
        <p>Constructed a speech dataset from synthesized data and implemented a trigger word detection model with over 90% accuracy. Trained a GRU(Gated Recurrent Units) to detect when someone has finished saying the word "activate".</p>
      </td>
    </tr>

    <tr onmouseout="car_detect_stop()" onmouseover="car_detect_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/car_detect_yolo.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='car_detect_image'><img src='images/car_detect_yolo.png' width="160"></div>
            <img src='images/car_detect_yolo.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function car_detect_start() {
            document.getElementById('car_detect_image').style.opacity = "1";
          }

          function car_detect_stop() {
            document.getElementById('car_detect_image').style.opacity = "0";
          }
          car_detect_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">
          <span class="papertitle">Car Detection with YOLO: You Only Look Once</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">code</a>
        <p>Implemented real-time object detection on a car dataset using the YOLO model, which was further improved using a U-net architecture. The YOLO model was stacked with Non-max suppression layers using IOU grid analysis to obtain the most accurate boundary boxes.</p>
      </td>
    </tr>

    <tr onmouseout="nst_stop()" onmouseover="nst_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <a href='images/nst.png' target="_blank" rel="noopener">
          <div class="one">
            <div class="two" id='nst_image'><img src='images/nst.png' width="160"></div>
            <img src='images/nst.png' width="160">
          </div>
        </a>
        <script type="text/javascript">
          function nst_start() {
            document.getElementById('nst_image').style.opacity = "1";
          }

          function nst_stop() {
            document.getElementById('nst_image').style.opacity = "0";
          }
          nst_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">
          <span class="papertitle">Art Generation with Neural Style Transfer</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">code</a>
        <p>Used transfer learning on the VGG-19 network to generate new artistic images. Implemented a cost function that minimizes the content and style cost by running both the images through the pre-trained VGG-19 model.</p>
      </td>
    </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you for visiting my site, and if you have any inputs, I would love to hear from you!
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron.github.io" target="_blank">source code</a>. Do not scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website — use the github code instead.
                </p>
              </td>
            </tr>
          </tbody></table>
          <div style="width:80px; height:80px; overflow:hidden;">
            <div style="transform: scale(0.25); transform-origin: top left;">
              <script type="text/javascript" id="clustrmaps"
                src="//clustrmaps.com/map_v2.js?d=GigM3hQ3XGh2hcg8VNPfJ5l7ogxZPVItB6KgyRjL1tM&cl=ffffff&w=200">
              </script>
            </div>
          </div>
        </td>
      </tr>
    </table>
    </body>
</html>
